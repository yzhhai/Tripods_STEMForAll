{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Two\n",
    "\n",
    "In this project, we will explore the concept of VC dimension in a series of learning examples.\n",
    "\n",
    "The goals of the project is to\n",
    "\n",
    "1) understand how knowledge about the VC dimension of hypothesis class plays a role in determining the sample complexity to ensure a probably approximately correct(PAC) learning under the realizability assumption;\n",
    "\n",
    "2) conversely, how the sample complexity for a PAC learning implies about the VC dimension of hypothesis class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background review:\n",
    "\n",
    "We first recall some important definitions in the setting.\n",
    "\n",
    "#### Definition 1 (Restriction of $\\mathcal{H}$ to $C$):\n",
    "Let $\\mathcal{H}$ be a class of functions from $X$ to $\\{0, 1\\}$ and let $C = \\{c_1, . . . , c_m\\} \\subseteq X.$ The restriction of $\\mathcal{H}$ to $C$ is the set of functions from $C$ to $\\{0, 1\\}$ that can be derived from $\\mathcal{H}$. That is,\n",
    "$$\\mathcal{H}_C = \\{(h(c_1), . . . ,h(c_m)) : h \\in H\\},$$\n",
    "where we represent each function from $C$ to $\\{0, 1\\}$ as a vector in $\\{0, 1\\}^{|C|}$.\n",
    "\n",
    "#### Definition 2 (Shattering): \n",
    "A hypothesis class $\\mathcal{H}$ shatters a finite set $C \\rightarrow X $ if the restriction of $\\mathcal{H}$ to $C$ is the set of all functions from $C$ to $\\{0, 1\\}$. That is, $|\\mathcal{H}_C| = 2^{|C|}.$\n",
    "\n",
    "#### Defintion 3 (VC dimension): \n",
    "The VC-dimension of a hypothesis class $\\mathcal{H}$,denoted $VCdim(\\mathcal{H})$, is the maximal size of a set $C \\subseteq X$ that can be shattered by $\\mathcal{H}$. If $\\mathcal{H}$ can shatter sets of arbitrarily large size we say that $\\mathcal{H}$ has infinite VC-dimension.\n",
    "\n",
    "A fundamental theorem that will be in use is the following which indicates the relation between the VC dimension and PAC learnablity:\n",
    "\n",
    "#### Theorem 1 (The Fundamental Theorem of Statistical Learning):\n",
    "Let $\\mathcal{H}$ be a hypothesis class of functions from a domain $X$ to $\\{0, 1\\}$ and let the loss function be the $0 − 1$ loss. Then, the following are equivalent:\n",
    "1. $\\mathcal{H}$ has the uniform convergence property.\n",
    "2. Any ERM rule is a successful agnostic PAC learner for $\\mathcal{H}$.\n",
    "3. $\\mathcal{H}$ is agnostic PAC learnable.\n",
    "4. $\\mathcal{H}$ is PAC learnable.\n",
    "5. Any ERM rule is a successful PAC learner for $\\mathcal{H}$.\n",
    "6. $\\mathcal{H}$ has a finite VC-dimension.\n",
    "\n",
    "The uniform convergence property of a hypothesis class with finite VC-dimension can be quantified further which will play an important role in this project:\n",
    "\n",
    "#### Theorem 2 (The Fundamental Theorem of Statistical Learning – Quantitative Version):\n",
    "Let $\\mathcal{H}$ be a hypothesis class of functions from a domain $X$ to $\\{0, 1\\}$\n",
    "and let the loss function be the $0 − 1$ loss. Assume that $VCdim(H) = d < \\infty.$\n",
    "Then, there are absolute constants $C_1, C_2$ such that: ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1:\n",
    "\n",
    "Let $X= \\{1, 2, \\ldots, 10^6\\}$ and $\\mathcal{H}$ be a class of threshold functions $\\{x \\mapsto \\mathbb{1}_{x \\leq a}: a \\in \\mathbb{R}\\}$. You can draw samples randomly from the data set provided. Please learn the samples drawn using the ERM rule.\n",
    "\n",
    "#### Questions:\n",
    "1) Given the finiteness of the hypothesis class, what can you say immediately about the uppoer bound for VC dimesion of $\\mathcal{H}$?\n",
    "\n",
    "2) Can you improve the trivial bound and find the exact VC dimension of $\\mathcal{H}$?\n",
    "\n",
    "3) Based on the knowledge of VC dimension of $\\mathcal{H}$, how many samples do you need to draw to ensure that the output hypothesis is probably approximately accurate with 98% accuracy and 95% confidence?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: \n",
    "\n",
    "Let $\\mathcal{H}$ be a class of linear classifiers over $\\mathbb{R}^2$. Please learn the samples drawn from the data set using the ERM rule.  \n",
    "\n",
    "#### Questions:\n",
    "1) What is the VC dimension of $\\mathcal{H}$? \n",
    "\n",
    "2) How many samples do you need to draw to ensure that the output hypothesis is probably approximately accurate with 98% accuracy and 95% confidence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: \n",
    "\n",
    "Let $\\mathcal{H}$ be a class of functions over $\\mathbb{R}$ defined by $\\{x \\mapsto \\lceil{\\sin(\\theta x)}\\rceil: \\theta \\in \\mathbb{R}\\}$. Please learn the samples drawn from the data set using the ERM rule.  \n",
    "\n",
    "#### Questions:\n",
    "1) Can you answer the same questions posed in Example 2? If yes, how does it explain the learning experiments and their accuracy? If not, could you guess from the experiments on the VC-dimension of $\\mathcal{H}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
